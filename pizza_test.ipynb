{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w207 Final Project\n",
    "Random Acts of Pizza Kaggle Competition Project\n",
    "\n",
    "Using a dataset of 4040 Reddit requests for pizza, we attempt to build a machine learning model that predicts if a request for pizza results in successfully receiving a pizza.\n",
    "\n",
    "We investigate several types of models that we import below from the sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import *\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we use pandas to read the training set and test set in json file format from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Download training and test datasets.\n",
    "pd_train = pd.read_json('https://raw.githubusercontent.com/mdemaster/w207_Final_Project/master/train.json', orient='columns')\n",
    "pd_test = pd.read_json('https://raw.githubusercontent.com/mdemaster/w207_Final_Project/master/test.json', orient='columns')\n",
    "\n",
    "#Move label field 'requester_received_pizza' to first field.\n",
    "pizza = pd_train['requester_received_pizza']\n",
    "pd_train.drop(labels=['requester_received_pizza'], axis=1,inplace = True)\n",
    "pd_train.insert(0, 'requester_received_pizza', pizza)\n",
    "\n",
    "#Create numpy arrays datasets\n",
    "np_test = np.array(pd_test)\n",
    "np_train = np.array(pd_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We discovered that the test set only has 17 fields compared to the 32 present in the training set.  The test set also doesn't have a field titled, \"requestor_received_pizza\", which is the field we are using for the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training set fields: 32                               Number of test set fields:  17                                                     \n",
      "===============================================================================================================\n",
      "requester_received_pizza                                        giver_username_if_known\n",
      "giver_username_if_known                                         request_id\n",
      "number_of_downvotes_of_request_at_retrieval                     request_text_edit_aware\n",
      "number_of_upvotes_of_request_at_retrieval                       request_title\n",
      "post_was_edited                                                 requester_account_age_in_days_at_request\n",
      "request_id                                                      requester_days_since_first_post_on_raop_at_request\n",
      "request_number_of_comments_at_retrieval                         requester_number_of_comments_at_request\n",
      "request_text                                                    requester_number_of_comments_in_raop_at_request\n",
      "request_text_edit_aware                                         requester_number_of_posts_at_request\n",
      "request_title                                                   requester_number_of_posts_on_raop_at_request\n",
      "requester_account_age_in_days_at_request                        requester_number_of_subreddits_at_request\n",
      "requester_account_age_in_days_at_retrieval                      requester_subreddits_at_request\n",
      "requester_days_since_first_post_on_raop_at_request              requester_upvotes_minus_downvotes_at_request\n",
      "requester_days_since_first_post_on_raop_at_retrieval            requester_upvotes_plus_downvotes_at_request\n",
      "requester_number_of_comments_at_request                         requester_username\n",
      "requester_number_of_comments_at_retrieval                       unix_timestamp_of_request\n",
      "requester_number_of_comments_in_raop_at_request                 unix_timestamp_of_request_utc\n",
      "requester_number_of_comments_in_raop_at_retrieval               \n",
      "requester_number_of_posts_at_request                            \n",
      "requester_number_of_posts_at_retrieval                          \n",
      "requester_number_of_posts_on_raop_at_request                    \n",
      "requester_number_of_posts_on_raop_at_retrieval                  \n",
      "requester_number_of_subreddits_at_request                       \n",
      "requester_subreddits_at_request                                 \n",
      "requester_upvotes_minus_downvotes_at_request                    \n",
      "requester_upvotes_minus_downvotes_at_retrieval                  \n",
      "requester_upvotes_plus_downvotes_at_request                     \n",
      "requester_upvotes_plus_downvotes_at_retrieval                   \n",
      "requester_user_flair                                            \n",
      "requester_username                                              \n",
      "unix_timestamp_of_request                                       \n",
      "unix_timestamp_of_request_utc                                   \n"
     ]
    }
   ],
   "source": [
    "#Print illustration of field differences in training and test sets.\n",
    "train_cols=str(len(pd_train.columns.values))\n",
    "test_cols=str(len(pd_test.columns.values))\n",
    "\n",
    "print 'Number of training set fields:',train_cols.ljust(31,' '),' Number of test set fields: ',test_cols.ljust(55,' ')\n",
    "print '==============================================================================================================='\n",
    "for i in range(len(pd_train.columns.values)):\n",
    "    train=pd_train.columns.values[i]\n",
    "    if i<len(pd_test.columns.values):\n",
    "        test=pd_test.columns.values[i]\n",
    "    else:\n",
    "        test=''\n",
    "    print train.ljust(55,' '), '       ', test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of these discrepancies in the given test data, we decided to split the train_data into a smaller training set, a dev set, and a test set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape:  (4040, 31)\n",
      "label shape: (4040,)\n"
     ]
    }
   ],
   "source": [
    "#Split dataset into predictor data,X, and labels,Y.\n",
    "X = np_train[:,1:]\n",
    "Y = np_train[:,0]\n",
    "\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "X, Y = X[shuffle], Y[shuffle]\n",
    "\n",
    "#Print data and label array shapes to confirm they are the same.\n",
    "print 'data shape: ', X.shape\n",
    "print 'label shape:', Y.shape\n",
    "\n",
    "#Assign first half of data to training set, 3rd quarter of data to dev set, and 4th quarter of data to test set.\n",
    "l=len(X)\n",
    "train_data, train_labels = X[:l/2], np.where(Y[:l/2][:]==True,1,0)\n",
    "dev_data, dev_labels = X[l/2:(3*l)/4], np.where(Y[l/2:(3*l)/4][:]==True,1,0)\n",
    "test_data, test_labels = X[(3*l)/4:], np.where(Y[(3*l)/4:][:]==True,1,0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After examining the dataset fields, we see that there are a number of scalar fields with highly variable distributions.  In order to normalize this information and utilize it for a Gaussian Mixture Model, we create new train, dev, and test sets with binarized fields with a uniform distrubtion binning criteria.\n",
    "\n",
    "For the first Binarize function, we started off with three bins per scalar field and even splits in the data ranges for the bin boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create dictionary of scalar field numbers and their corresponding number of bins.\n",
    "fields_bins = {\n",
    "1:3, 2:3, 5:3, 9:3, 10:3, 11:3, 12:3, 13:3, 14:3, 15:3, 16:3, 17:3, 18:3, 19:3, 20:3, 21:3, 23:3, 24:3, 25:3, 26:3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-393-c736b2606948>:51: SyntaxWarning: name 'bin_train' is assigned to before global declaration\n",
      "  global bin_train,bin_dev,bin_test\n",
      "<ipython-input-393-c736b2606948>:51: SyntaxWarning: name 'bin_dev' is assigned to before global declaration\n",
      "  global bin_train,bin_dev,bin_test\n",
      "<ipython-input-393-c736b2606948>:51: SyntaxWarning: name 'bin_test' is assigned to before global declaration\n",
      "  global bin_train,bin_dev,bin_test\n"
     ]
    }
   ],
   "source": [
    "def Binarize1(fields_bins,X):\n",
    "    #Binarize Training Data\n",
    "    train_data=X[:l/2]\n",
    "    s=train_data.shape[0]\n",
    "    bin_train=np.where(train_data[:,0]==u'N/A',0.,1.).reshape(s,1)\n",
    "\n",
    "    for f, b in fields_bins.items():\n",
    "        col=train_data[:,f]\n",
    "        sort=np.sort(col)\n",
    "        bin_train = np.column_stack((bin_train,np.where(col<sort[s/b],1.,0.).reshape(s,1)))\n",
    "        hist_bins=[0]\n",
    "        for i in range(b-2):\n",
    "            x=sort[(i+1)*s/b]\n",
    "            y=sort[(i+2)*s/b]\n",
    "            bin_train = np.column_stack((bin_train,np.where((col>=x) & (col<y),1.,0.).reshape(s,1)))\n",
    "            hist_bins=np.append(hist_bins,x)\n",
    "        hist_bins=np.append(hist_bins,sort[-1])\n",
    "        bin_train = np.column_stack((bin_train,np.where(col>=sort[(b-1)*s/b],1.,0.).reshape(s,1)))\n",
    "        #print pd_train.columns.values[f+1],'__histogram bins: ',hist_bins\n",
    "\n",
    "    #Binarize Dev Data\n",
    "    dev_data = X[l/2:(3*l)/4]\n",
    "    s1=dev_data.shape[0]\n",
    "    bin_dev=np.where(dev_data[:,0]==u'N/A',0.,1.).reshape(s1,1)\n",
    "\n",
    "    for f, b in fields_bins.items(): \n",
    "        col=dev_data[:,f]\n",
    "        sort=np.sort(col)\n",
    "        bin_dev = np.column_stack((bin_dev,np.where(col<sort[s1/b],1.,0.).reshape(s1,1)))\n",
    "        for i in range(b-2):\n",
    "            x=sort[(i+1)*s1/b]\n",
    "            y=sort[(i+2)*s1/b]\n",
    "            bin_dev = np.column_stack((bin_dev,np.where((col>=x) & (col<y),1.,0.).reshape(s1,1)))\n",
    "        bin_dev = np.column_stack((bin_dev,np.where(col>=sort[(b-1)*s1/b],1.,0.).reshape(s1,1)))\n",
    "\n",
    "    #Binarize Test Data\n",
    "    test_data = X[(3*l)/4:]\n",
    "    s2=test_data.shape[0]\n",
    "    bin_test=np.where(test_data[:,0]==u'N/A',0.,1.).reshape(s2,1)\n",
    "\n",
    "    for f, b in fields_bins.items():\n",
    "        col=test_data[:,f]\n",
    "        sort=np.sort(col)\n",
    "        bin_test = np.column_stack((bin_test,np.where(col<sort[s2/b],1.,0.).reshape(s2,1)))\n",
    "        for i in range(b-2):\n",
    "            x=sort[(i+1)*s2/b]\n",
    "            y=sort[(i+2)*s2/b]\n",
    "            bin_test = np.column_stack((bin_test,np.where((col>=x) & (col<y),1.,0.).reshape(s2,1)))\n",
    "        bin_test = np.column_stack((bin_test,np.where(col>=sort[(b-1)*s2/b],1.,0.).reshape(s2,1)))\n",
    "    \n",
    "    global bin_train,bin_dev,bin_test\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we built a function to take the binned training data and binned dev data and run a Gaussian mixture model on this data.  The function loops through the four covariance types, a range of PCA components, and a range of GMM components to find the model with the most accurate fit. The top 10 models are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mixture_model(bin_train,bin_dev,bin_test):\n",
    "    experiments=[]\n",
    "\n",
    "    for c_type in ['spherical', 'diag', 'tied', 'full']:\n",
    "        for p_comp in range(1,60):\n",
    "            for g_comp in range(1,9):\n",
    "\n",
    "                params=((3+p_comp)*g_comp)*2\n",
    "                if params<=100:\n",
    "                    #Run PCA with two components\n",
    "                    pca = PCA(p_comp)\n",
    "\n",
    "                    #Assign train data and labels to x and y for fitting and transforming.\n",
    "                    x=bin_train\n",
    "                    y=train_labels\n",
    "\n",
    "                    #Transform train data and fit it with labels to 2-component projected PCA model.\n",
    "                    proj_train=pca.fit(x,y).transform(x)\n",
    "\n",
    "                    #Transform test data to 2-component projected PCA model\n",
    "                    proj_test=pca.transform(bin_dev)\n",
    "\n",
    "                    #Filter Projected data by positive examples.\n",
    "                    pos=proj_train[y == 1]\n",
    "                    neg=proj_train[y == 0]\n",
    "\n",
    "                    #Fit GMM model to positive and negative train datasets.\n",
    "                    gmm_pos = GMM(n_components=g_comp, covariance_type=c_type).fit(pos)\n",
    "                    gmm_neg = GMM(n_components=g_comp, covariance_type=c_type).fit(neg)\n",
    "\n",
    "                    #Get positive and negative GMM model scores for test data.\n",
    "                    gmm_pos_score=np.array(gmm_pos.score(proj_test))\n",
    "                    gmm_neg_score=np.array(gmm_neg.score(proj_test))\n",
    "\n",
    "                    #Merge score arrays and check which score is greater to determine positive/negative classes.\n",
    "                    point_scores=np.column_stack((gmm_pos_score,gmm_neg_score))\n",
    "                    gmm_pred=np.where(point_scores[:,0]>point_scores[:,1], 1, 0)\n",
    "\n",
    "                    #Calculate accuracy of merged scoring model method by comparing predicted classes with test_labels.  \n",
    "                    accuracy = 1-np.mean(gmm_pred != dev_labels)\n",
    "\n",
    "                    #Append model parameters and scores to experiments list\n",
    "                    experiments.append([p_comp,g_comp,c_type,accuracy,params])\n",
    "\n",
    "    #Make experiments an array and sort by descending accuracy\n",
    "    experiments = np.array(experiments)\n",
    "    experiments = experiments[np.argsort(experiments[:, 3])[::-1]]\n",
    "\n",
    "    #Print results of experiments\n",
    "    print 'Top Scoring GMM Models'\n",
    "    print 'PCA Components  ||  GMM Components  ||  Covariance Type  ||  Accuracy  ||  Parameters'\n",
    "\n",
    "    for i in  experiments[:10]:\n",
    "        print '%14s  ||' %(i[0])+'%16s  ||' %(i[1])+'%17s  ||' %(i[2])+'  %.4f    ||' %(float(i[3]))+'%10s ' %(i[4])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we see that the GMM model on the binned datasets shows improved accuracy at around 80% on the test set and 81% on the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Scoring GMM Models\n",
      "PCA Components  ||  GMM Components  ||  Covariance Type  ||  Accuracy  ||  Parameters\n",
      "            27  ||               1  ||             tied  ||  0.8119    ||        60 \n",
      "            27  ||               1  ||             full  ||  0.8119    ||        60 \n",
      "            24  ||               1  ||             tied  ||  0.8099    ||        54 \n",
      "            24  ||               1  ||             full  ||  0.8099    ||        54 \n",
      "            25  ||               1  ||             tied  ||  0.8069    ||        56 \n",
      "            25  ||               1  ||             full  ||  0.8069    ||        56 \n",
      "            26  ||               1  ||             tied  ||  0.8059    ||        58 \n",
      "            26  ||               1  ||             full  ||  0.8059    ||        58 \n",
      "            28  ||               1  ||             tied  ||  0.8030    ||        62 \n",
      "            28  ||               1  ||             full  ||  0.8030    ||        62 \n"
     ]
    }
   ],
   "source": [
    "#Run mixture model on first binarized train set\n",
    "Binarize1(fields_bins,X)\n",
    "mixture_model(bin_train,bin_dev,bin_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a second binarize function using the numpy histogram function, which creates normalized binning boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-418-430688cd98d6>:43: SyntaxWarning: name 'bin_train' is assigned to before global declaration\n",
      "  global bin_train,bin_dev,bin_test\n",
      "<ipython-input-418-430688cd98d6>:43: SyntaxWarning: name 'bin_dev' is assigned to before global declaration\n",
      "  global bin_train,bin_dev,bin_test\n",
      "<ipython-input-418-430688cd98d6>:43: SyntaxWarning: name 'bin_test' is assigned to before global declaration\n",
      "  global bin_train,bin_dev,bin_test\n"
     ]
    }
   ],
   "source": [
    "def Binarize2(fields_bins,X):\n",
    "    binnum=3\n",
    "    \n",
    "    #Binarize Training Data\n",
    "    train_data=X[:l/2]\n",
    "    s=train_data.shape[0]\n",
    "    bin_train=np.where(train_data[:,0]==u'N/A',0.,1.).reshape(s,1)\n",
    "\n",
    "    for f, b in fields_bins.items():\n",
    "        col=train_data[:,f]\n",
    "        h=np.histogram(col,bins=binnum,normed=True)[1]\n",
    "        bin_train = np.column_stack((bin_train,np.where(col<h[1],1.,0.).reshape(s,1)))\n",
    "        for i in range(len(h)-2):\n",
    "            bin_train = np.column_stack((bin_train,np.where((col>=h[i+1]) & (col<h[i+2]),1.,0.).reshape(s,1)))\n",
    "        bin_train = np.column_stack((bin_train,np.where(col>=h[-2],1.,0.).reshape(s,1)))\n",
    "\n",
    "\n",
    "    #Binarize Dev Data\n",
    "    dev_data = X[l/2:(3*l)/4]\n",
    "    s1=dev_data.shape[0]\n",
    "    bin_dev=np.where(dev_data[:,0]==u'N/A',0.,1.).reshape(s1,1)\n",
    "\n",
    "    for f, b in fields_bins.items():\n",
    "        col=dev_data[:,f]\n",
    "        h=np.histogram(col,bins=binnum,normed=True)[1]\n",
    "        bin_dev = np.column_stack((bin_dev,np.where(col<h[1],1.,0.).reshape(s1,1)))\n",
    "        for i in range(len(h)-2):\n",
    "            bin_dev = np.column_stack((bin_dev,np.where((col>=h[i+1]) & (col<h[i+2]),1.,0.).reshape(s1,1)))\n",
    "        bin_dev = np.column_stack((bin_dev,np.where(col>=h[-2],1.,0.).reshape(s1,1)))\n",
    "\n",
    "    #Binarize Test Data\n",
    "    test_data = X[(3*l)/4:]\n",
    "    s2=test_data.shape[0]\n",
    "    bin_test=np.where(test_data[:,0]==u'N/A',0.,1.).reshape(s2,1)\n",
    "\n",
    "    for f, b in fields_bins.items():\n",
    "        col=test_data[:,f]\n",
    "        h=np.histogram(col,bins=binnum,normed=True)[1]\n",
    "        bin_test = np.column_stack((bin_test,np.where(col<h[1],1.,0.).reshape(s2,1)))\n",
    "        for i in range(len(h)-2):\n",
    "            bin_test = np.column_stack((bin_test,np.where((col>=h[i+1]) & (col<h[i+2]),1.,0.).reshape(s2,1)))\n",
    "        bin_test = np.column_stack((bin_test,np.where(col>=h[-2],1.,0.).reshape(s2,1)))\n",
    "    global bin_train,bin_dev,bin_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Binarize2() and the mixture_model functions, we see an improvement in accuracy on the dev set up to 85%, but there was no improvement on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Scoring GMM Models\n",
      "PCA Components  ||  GMM Components  ||  Covariance Type  ||  Accuracy  ||  Parameters\n",
      "             6  ||               5  ||             diag  ||  0.8475    ||        90 \n",
      "            14  ||               1  ||             full  ||  0.8446    ||        34 \n",
      "             5  ||               5  ||             diag  ||  0.8446    ||        80 \n",
      "            14  ||               1  ||             tied  ||  0.8446    ||        34 \n",
      "            15  ||               1  ||             full  ||  0.8426    ||        36 \n",
      "            15  ||               1  ||             tied  ||  0.8426    ||        36 \n",
      "            13  ||               3  ||             tied  ||  0.8426    ||        96 \n",
      "            14  ||               2  ||             tied  ||  0.8426    ||        68 \n",
      "             4  ||               7  ||             tied  ||  0.8416    ||        98 \n",
      "            15  ||               2  ||             tied  ||  0.8406    ||        72 \n"
     ]
    }
   ],
   "source": [
    "#Run mixture model on second binned train set\n",
    "Binarize2(fields_bins,X)\n",
    "mixture_model(bin_train,bin_dev,bin_test)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
