{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w207 Final Project\n",
    "Random Acts of Pizza Kaggle Competition Project\n",
    "\n",
    "Using a dataset of 4040 Reddit requests for pizza, we attempt to build a machine learning model that predicts if a request for pizza results in successfully receiving a pizza.\n",
    "\n",
    "We investigate several types of models that we import below from the sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import *\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we use pandas to read the training set and test set in json file format from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Download training and test datasets.\n",
    "pd_train = pd.read_json('https://raw.githubusercontent.com/mdemaster/w207_Final_Project/master/train.json', orient='columns')\n",
    "pd_test = pd.read_json('https://raw.githubusercontent.com/mdemaster/w207_Final_Project/master/test.json', orient='columns')\n",
    "\n",
    "#Move label field 'requester_received_pizza' to first field.\n",
    "pizza = pd_train['requester_received_pizza']\n",
    "pd_train.drop(labels=['requester_received_pizza'], axis=1,inplace = True)\n",
    "pd_train.insert(0, 'requester_received_pizza', pizza)\n",
    "\n",
    "#Create numpy arrays datasets\n",
    "np_test = np.array(pd_test)\n",
    "np_train = np.array(pd_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We discovered that the test set only has 17 fields compared to the 32 present in the training set.  The test set also doesn't have a field titled, \"requestor_received_pizza\", which is the field we are using for the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training set fields: 32                               Number of test set fields:  17                                                     \n",
      "===============================================================================================================\n",
      "requester_received_pizza                                        giver_username_if_known\n",
      "giver_username_if_known                                         request_id\n",
      "number_of_downvotes_of_request_at_retrieval                     request_text_edit_aware\n",
      "number_of_upvotes_of_request_at_retrieval                       request_title\n",
      "post_was_edited                                                 requester_account_age_in_days_at_request\n",
      "request_id                                                      requester_days_since_first_post_on_raop_at_request\n",
      "request_number_of_comments_at_retrieval                         requester_number_of_comments_at_request\n",
      "request_text                                                    requester_number_of_comments_in_raop_at_request\n",
      "request_text_edit_aware                                         requester_number_of_posts_at_request\n",
      "request_title                                                   requester_number_of_posts_on_raop_at_request\n",
      "requester_account_age_in_days_at_request                        requester_number_of_subreddits_at_request\n",
      "requester_account_age_in_days_at_retrieval                      requester_subreddits_at_request\n",
      "requester_days_since_first_post_on_raop_at_request              requester_upvotes_minus_downvotes_at_request\n",
      "requester_days_since_first_post_on_raop_at_retrieval            requester_upvotes_plus_downvotes_at_request\n",
      "requester_number_of_comments_at_request                         requester_username\n",
      "requester_number_of_comments_at_retrieval                       unix_timestamp_of_request\n",
      "requester_number_of_comments_in_raop_at_request                 unix_timestamp_of_request_utc\n",
      "requester_number_of_comments_in_raop_at_retrieval               \n",
      "requester_number_of_posts_at_request                            \n",
      "requester_number_of_posts_at_retrieval                          \n",
      "requester_number_of_posts_on_raop_at_request                    \n",
      "requester_number_of_posts_on_raop_at_retrieval                  \n",
      "requester_number_of_subreddits_at_request                       \n",
      "requester_subreddits_at_request                                 \n",
      "requester_upvotes_minus_downvotes_at_request                    \n",
      "requester_upvotes_minus_downvotes_at_retrieval                  \n",
      "requester_upvotes_plus_downvotes_at_request                     \n",
      "requester_upvotes_plus_downvotes_at_retrieval                   \n",
      "requester_user_flair                                            \n",
      "requester_username                                              \n",
      "unix_timestamp_of_request                                       \n",
      "unix_timestamp_of_request_utc                                   \n"
     ]
    }
   ],
   "source": [
    "#Print illustration of field differences in training and test sets.\n",
    "train_cols=str(len(pd_train.columns.values))\n",
    "test_cols=str(len(pd_test.columns.values))\n",
    "\n",
    "print 'Number of training set fields:',train_cols.ljust(31,' '),' Number of test set fields: ',test_cols.ljust(55,' ')\n",
    "print '==============================================================================================================='\n",
    "for i in range(len(pd_train.columns.values)):\n",
    "    train=pd_train.columns.values[i]\n",
    "    if i<len(pd_test.columns.values):\n",
    "        test=pd_test.columns.values[i]\n",
    "    else:\n",
    "        test=''\n",
    "    print train.ljust(55,' '), '       ', test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of these discrepancies in the given test data, we decided to split the train_data into a smaller training set, a dev set, and a test set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape:  (4040, 31)\n",
      "label shape: (4040,)\n"
     ]
    }
   ],
   "source": [
    "#Split dataset into predictor data,X, and labels,Y.\n",
    "X = np_train[:,1:]\n",
    "Y = np_train[:,0]\n",
    "\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "X, Y = X[shuffle], Y[shuffle]\n",
    "\n",
    "#Print data and label array shapes to confirm they are the same.\n",
    "print 'data shape: ', X.shape\n",
    "print 'label shape:', Y.shape\n",
    "\n",
    "#Assign first half of data to training set, 3rd quarter of data to dev set, and 4th quarter of data to test set.\n",
    "l=len(X)\n",
    "train_data, train_labels = X[:l/2], np.where(Y[:l/2][:]==True,1,0)\n",
    "dev_data, dev_labels = X[l/2:(3*l)/4], np.where(Y[l/2:(3*l)/4][:]==True,1,0)\n",
    "test_data, test_labels = X[(3*l)/4:], np.where(Y[(3*l)/4:][:]==True,1,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2020, 6474)\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "def better_preprocessor(s):\n",
    "    repl = re.sub('&', ' and ', s)\n",
    "    repl = repl.lower()\n",
    "    repl = repl.replace('0',' zero ')\n",
    "    repl = repl.replace('1',' one ')\n",
    "    repl = repl.replace('2',' two ')\n",
    "    repl = repl.replace('3',' three ')\n",
    "    repl = repl.replace('4',' four ')\n",
    "    repl = repl.replace('5',' five ')\n",
    "    repl = repl.replace('6',' six ')\n",
    "    repl = repl.replace('7',' seven ')\n",
    "    repl = repl.replace('8',' eight ')\n",
    "    repl = repl.replace('9',' nine ')\n",
    "    repl = re.sub('[^a-z]+',' ', repl)\n",
    "    return repl\n",
    "\n",
    "def get_predicted_probability(data):\n",
    "    tfidvec = TfidfVectorizer(preprocessor=better_preprocessor, ngram_range=(3,5),max_df=0.5, min_df=3);\n",
    "    train_X = tfidvec.fit_transform(data)\n",
    "    return train_X\n",
    "fitted_text_data = get_predicted_probability(train_data[:,7])\n",
    "print fitted_text_data.shape\n",
    "# print('Sum of train(Got pizza)', sum(trainlabels),' (Didn\\'t get pizza:)', len(trainlabels) - sum(trainlabels))\n",
    "# print('Sum of dev(Got pizza)', sum(devlabels),' (Didn\\'t get pizza:)', len(devlabels) - sum(devlabels))\n",
    "# print('Sum of test(Got pizza)', sum(testlabels),' (Didn\\'t get pizza:)', len(testlabels) - sum(testlabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After examining the dataset fields, we see that there are a number of scalar fields with highly variable distributions.  In order to normalize this information and utilize it for a Gaussian Mixture Model, we create new train, dev, and test sets with binarized fields with a uniform distrubtion binning criteria.\n",
    "\n",
    "For the first Binarize function, we started off with three bins per scalar field and even splits in the data ranges for the bin boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create dictionary of scalar field numbers and their corresponding number of bins.\n",
    "fields_bins = {\n",
    "1:3, 2:3, 5:3, 9:3, 10:3, 11:3, 12:3, 13:3, 14:3, 15:3, 16:3, 17:3, 18:3, 19:3, 20:3, 21:3, 23:3, 24:3, 25:3, 26:3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Binarize1(fields_bins,X):\n",
    "    #Binarize Training Data\n",
    "    train_data=X[:l/2]\n",
    "    s=train_data.shape[0]\n",
    "    bin_train=np.where(train_data[:,0]==u'N/A',0.,1.).reshape(s,1)\n",
    "\n",
    "    \n",
    "    # tfid and log clg, get top positive weights and top negative weights\n",
    "    tfidvec = TfidfVectorizer(preprocessor=better_preprocessor, ngram_range=(3,5),max_df=0.5, min_df=3);\n",
    "    train_X = tfidvec.fit_transform(train_data[:,7])\n",
    "    log_clf = LogisticRegression(C=100)\n",
    "    log_clf = log_clf.fit(train_X, train_labels)\n",
    "    features = tfidvec.get_feature_names();\n",
    "    weights = log_clf.coef_\n",
    "    \n",
    "    print 'positive weights:'\n",
    "    weight_indexes = []\n",
    "    positive_features = []\n",
    "    weight_index = weights[0].argsort()[-10:][::-1].tolist()\n",
    "    weight_indexes += (weight_index)    \n",
    "    for i in range(len(weight_indexes)):\n",
    "        index = weight_indexes[i]\n",
    "        positive_features.append(features[index])\n",
    "#             print 'Feature Name:', features[index]\n",
    "#             print weights[0][index]\n",
    "#             print ''\n",
    "    print positive_features\n",
    "    print ''\n",
    "    print 'negative weights:'\n",
    "    weight_indexes = []\n",
    "    negative_features = []\n",
    "    weight_index = weights[0].argsort()[:10].tolist()\n",
    "    weight_indexes += (weight_index)    \n",
    "    for i in range(len(weight_indexes)):\n",
    "        index = weight_indexes[i]\n",
    "        negative_features.append(features[index])\n",
    "#             print 'Feature Name:', features[index]\n",
    "#             print weights[0][index]\n",
    "#             print ''\n",
    "    print negative_features\n",
    "    print ''\n",
    "\n",
    "    \n",
    "    for f, b in fields_bins.items():\n",
    "        col=train_data[:,f]\n",
    "        sort=np.sort(col)\n",
    "        bin_train = np.column_stack((bin_train,np.where(col<sort[s/b],1.,0.).reshape(s,1)))\n",
    "        hist_bins=[0]\n",
    "        for i in range(b-2):\n",
    "            x=sort[(i+1)*s/b]\n",
    "            y=sort[(i+2)*s/b]\n",
    "            bin_train = np.column_stack((bin_train,np.where((col>=x) & (col<y),1.,0.).reshape(s,1)))\n",
    "            hist_bins=np.append(hist_bins,x)\n",
    "        hist_bins=np.append(hist_bins,sort[-1])\n",
    "        bin_train = np.column_stack((bin_train,np.where(col>=sort[(b-1)*s/b],1.,0.).reshape(s,1)))\n",
    "    \n",
    "    for pos_f in positive_features:\n",
    "        pos_f_arr = []\n",
    "        for line in train_data[:,7]:\n",
    "            pos_f_arr.append(np.where(pos_f in line, 1, 0))\n",
    "        bin_train = np.column_stack((bin_train, pos_f_arr))\n",
    "    for neg_f in negative_features:\n",
    "        neg_f_arr = []\n",
    "        for line in train_data[:,7]:\n",
    "            neg_f_arr.append(np.where(neg_f in line, 1, 0))\n",
    "        bin_train = np.column_stack((bin_train, neg_f_arr))\n",
    "        #print pd_train.columns.values[f+1],'__histogram bins: ',hist_bins\n",
    "    #bin_train = np.column_stack((bin_train, pred))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Binarize Dev Data\n",
    "    dev_data = X[l/2:(3*l)/4]\n",
    "    s1=dev_data.shape[0]\n",
    "    bin_dev=np.where(dev_data[:,0]==u'N/A',0.,1.).reshape(s1,1)\n",
    "\n",
    "    for f, b in fields_bins.items(): \n",
    "        col=dev_data[:,f]\n",
    "        sort=np.sort(col)\n",
    "        bin_dev = np.column_stack((bin_dev,np.where(col<sort[s1/b],1.,0.).reshape(s1,1)))\n",
    "        for i in range(b-2):\n",
    "            x=sort[(i+1)*s1/b]\n",
    "            y=sort[(i+2)*s1/b]\n",
    "            bin_dev = np.column_stack((bin_dev,np.where((col>=x) & (col<y),1.,0.).reshape(s1,1)))\n",
    "        bin_dev = np.column_stack((bin_dev,np.where(col>=sort[(b-1)*s1/b],1.,0.).reshape(s1,1)))\n",
    "    for pos_f in positive_features:\n",
    "        pos_f_arr = []\n",
    "        for line in dev_data[:,7]:\n",
    "            pos_f_arr.append(np.where(pos_f in line, 1, 0))\n",
    "        bin_dev = np.column_stack((bin_dev, pos_f_arr))\n",
    "    for neg_f in negative_features:\n",
    "        neg_f_arr = []\n",
    "        for line in dev_data[:,7]:\n",
    "            neg_f_arr.append(np.where(neg_f in line, 1, 0))\n",
    "        bin_dev = np.column_stack((bin_dev, neg_f_arr))\n",
    "        \n",
    "        \n",
    "    #Binarize Test Data\n",
    "    test_data = X[(3*l)/4:]\n",
    "    s2=test_data.shape[0]\n",
    "    bin_test=np.where(test_data[:,0]==u'N/A',0.,1.).reshape(s2,1)\n",
    "\n",
    "    for f, b in fields_bins.items():\n",
    "        col=test_data[:,f]\n",
    "        sort=np.sort(col)\n",
    "        bin_test = np.column_stack((bin_test,np.where(col<sort[s2/b],1.,0.).reshape(s2,1)))\n",
    "        for i in range(b-2):\n",
    "            x=sort[(i+1)*s2/b]\n",
    "            y=sort[(i+2)*s2/b]\n",
    "            bin_test = np.column_stack((bin_test,np.where((col>=x) & (col<y),1.,0.).reshape(s2,1)))\n",
    "        bin_test = np.column_stack((bin_test,np.where(col>=sort[(b-1)*s2/b],1.,0.).reshape(s2,1)))\n",
    "    \n",
    "    for pos_f in positive_features:\n",
    "        pos_f_arr = []\n",
    "        for line in test_data[:,7]:\n",
    "            pos_f_arr.append(np.where(pos_f in line, 1, 0))\n",
    "        bin_test = np.column_stack((bin_test, pos_f_arr))\n",
    "    for neg_f in negative_features:\n",
    "        neg_f_arr = []\n",
    "        for line in test_data[:,7]:\n",
    "            neg_f_arr.append(np.where(neg_f in line, 1, 0))\n",
    "        bin_test = np.column_stack((bin_test, neg_f_arr))\n",
    "        \n",
    "#     test_X = tfidvec.fit_transform(test_data[:,7])\n",
    "#     pred = log_clf.predict(test_X)\n",
    "#     print 'test pred:',pred.shape, bin_test.shape\n",
    "#     bin_test = np.column_stack((bin_test, pred))\n",
    "    \n",
    "#     global bin_train,bin_dev,bin_test\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we built a function to take the binned training data and binned dev data and run a Gaussian mixture model on this data.  The function loops through the four covariance types, a range of PCA components, and a range of GMM components to find the model with the most accurate fit. The top 10 models are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mixture_model(bin_train,bin_dev,bin_test):\n",
    "    experiments=[]\n",
    "\n",
    "    for c_type in ['spherical', 'diag', 'tied', 'full']:\n",
    "        for p_comp in range(1,60):\n",
    "            for g_comp in range(1,9):\n",
    "\n",
    "                params=((3+p_comp)*g_comp)*2\n",
    "                if params<=100:\n",
    "                    #Run PCA with two components\n",
    "                    pca = PCA(p_comp)\n",
    "\n",
    "                    #Assign train data and labels to x and y for fitting and transforming.\n",
    "                    x=bin_train\n",
    "                    y=train_labels\n",
    "\n",
    "                    #Transform train data and fit it with labels to 2-component projected PCA model.\n",
    "                    proj_train=pca.fit(x,y).transform(x)\n",
    "\n",
    "                    #Transform test data to 2-component projected PCA model\n",
    "                    proj_test=pca.transform(bin_dev)\n",
    "\n",
    "                    #Filter Projected data by positive examples.\n",
    "                    pos=proj_train[y == 1]\n",
    "                    neg=proj_train[y == 0]\n",
    "\n",
    "                    #Fit GMM model to positive and negative train datasets.\n",
    "                    gmm_pos = GMM(n_components=g_comp, covariance_type=c_type).fit(pos)\n",
    "                    gmm_neg = GMM(n_components=g_comp, covariance_type=c_type).fit(neg)\n",
    "\n",
    "                    #Get positive and negative GMM model scores for test data.\n",
    "                    gmm_pos_score=np.array(gmm_pos.score(proj_test))\n",
    "                    gmm_neg_score=np.array(gmm_neg.score(proj_test))\n",
    "\n",
    "                    #Merge score arrays and check which score is greater to determine positive/negative classes.\n",
    "                    point_scores=np.column_stack((gmm_pos_score,gmm_neg_score))\n",
    "                    gmm_pred=np.where(point_scores[:,0]>point_scores[:,1], 1, 0)\n",
    "\n",
    "                    #Calculate accuracy of merged scoring model method by comparing predicted classes with test_labels.  \n",
    "                    accuracy = 1-np.mean(gmm_pred != dev_labels)\n",
    "\n",
    "                    #Append model parameters and scores to experiments list\n",
    "                    experiments.append([p_comp,g_comp,c_type,accuracy,params])\n",
    "\n",
    "    #Make experiments an array and sort by descending accuracy\n",
    "    experiments = np.array(experiments)\n",
    "    experiments = experiments[np.argsort(experiments[:, 3])[::-1]]\n",
    "\n",
    "    #Print results of experiments\n",
    "    print 'Top Scoring GMM Models'\n",
    "    print 'PCA Components  ||  GMM Components  ||  Covariance Type  ||  Accuracy  ||  Parameters'\n",
    "\n",
    "    for i in  experiments[:10]:\n",
    "        print '%14s  ||' %(i[0])+'%16s  ||' %(i[1])+'%17s  ||' %(i[2])+'  %.4f    ||' %(float(i[3]))+'%10s ' %(i[4])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we see that the GMM model on the binned datasets shows improved accuracy at around 80% on the test set and 81% on the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive weights:\n",
      "[u'to ask for', u'just your average', u'will pay forward', u'some pizza love', u'really make my day', u'and will be', u'but don think', u'in the title', u'help guy out', u'right now can']\n",
      "\n",
      "negative weights:\n",
      "[u'started new job', u'low on cash', u'like to have', u'would be appreciated', u'food for the next', u'for the first', u'make it to', u'just moved to', u'mother of two', u'are not starving']\n",
      "\n",
      "Top Scoring GMM Models\n",
      "PCA Components  ||  GMM Components  ||  Covariance Type  ||  Accuracy  ||  Parameters\n",
      "            29  ||               1  ||             full  ||  0.8386    ||        64 \n",
      "            29  ||               1  ||             tied  ||  0.8386    ||        64 \n",
      "            26  ||               1  ||             full  ||  0.8366    ||        58 \n",
      "            26  ||               1  ||             tied  ||  0.8366    ||        58 \n",
      "            47  ||               1  ||             full  ||  0.8356    ||       100 \n",
      "            35  ||               1  ||             tied  ||  0.8356    ||        76 \n",
      "            27  ||               1  ||             tied  ||  0.8356    ||        60 \n",
      "            28  ||               1  ||             tied  ||  0.8356    ||        62 \n",
      "            32  ||               1  ||             tied  ||  0.8356    ||        70 \n",
      "            33  ||               1  ||             tied  ||  0.8356    ||        72 \n"
     ]
    }
   ],
   "source": [
    "#Run mixture model on first binarized train set\n",
    "Binarize1(fields_bins,X)\n",
    "mixture_model(bin_train,bin_dev,bin_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a second binarize function using the numpy histogram function, which creates normalized binning boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-133-d4f3f8622021>:112: SyntaxWarning: name 'bin_train' is assigned to before global declaration\n",
      "  global bin_train,bin_dev,bin_test\n",
      "<ipython-input-133-d4f3f8622021>:112: SyntaxWarning: name 'bin_dev' is assigned to before global declaration\n",
      "  global bin_train,bin_dev,bin_test\n",
      "<ipython-input-133-d4f3f8622021>:112: SyntaxWarning: name 'bin_test' is assigned to before global declaration\n",
      "  global bin_train,bin_dev,bin_test\n"
     ]
    }
   ],
   "source": [
    "def Binarize2(fields_bins,X):\n",
    "    binnum=3\n",
    "    \n",
    "    #Binarize Training Data\n",
    "    train_data=X[:l/2]\n",
    "    s=train_data.shape[0]\n",
    "    bin_train=np.where(train_data[:,0]==u'N/A',0.,1.).reshape(s,1)\n",
    "\n",
    "    # tfid and log clg, get top positive weights and top negative weights\n",
    "    tfidvec = TfidfVectorizer(preprocessor=better_preprocessor, ngram_range=(3,5),max_df=0.5, min_df=3);\n",
    "    train_X = tfidvec.fit_transform(train_data[:,7])\n",
    "    log_clf = LogisticRegression(C=100)\n",
    "    log_clf = log_clf.fit(train_X, train_labels)\n",
    "    features = tfidvec.get_feature_names();\n",
    "    weights = log_clf.coef_\n",
    "    \n",
    "    print 'positive weights:'\n",
    "    weight_indexes = []\n",
    "    positive_features = []\n",
    "    weight_index = weights[0].argsort()[-10:][::-1].tolist()\n",
    "    weight_indexes += (weight_index)    \n",
    "    for i in range(len(weight_indexes)):\n",
    "        index = weight_indexes[i]\n",
    "        positive_features.append(features[index])\n",
    "#             print 'Feature Name:', features[index]\n",
    "#             print weights[0][index]\n",
    "#             print ''\n",
    "    print positive_features\n",
    "    print ''\n",
    "    print 'negative weights:'\n",
    "    weight_indexes = []\n",
    "    negative_features = []\n",
    "    weight_index = weights[0].argsort()[:10].tolist()\n",
    "    weight_indexes += (weight_index)    \n",
    "    for i in range(len(weight_indexes)):\n",
    "        index = weight_indexes[i]\n",
    "        negative_features.append(features[index])\n",
    "#             print 'Feature Name:', features[index]\n",
    "#             print weights[0][index]\n",
    "#             print ''\n",
    "    print negative_features\n",
    "    print ''\n",
    "    \n",
    "    \n",
    "    for f, b in fields_bins.items():\n",
    "        col=train_data[:,f]\n",
    "        h=np.histogram(col,bins=binnum,normed=True)[1]\n",
    "        bin_train = np.column_stack((bin_train,np.where(col<h[1],1.,0.).reshape(s,1)))\n",
    "        for i in range(len(h)-2):\n",
    "            bin_train = np.column_stack((bin_train,np.where((col>=h[i+1]) & (col<h[i+2]),1.,0.).reshape(s,1)))\n",
    "        bin_train = np.column_stack((bin_train,np.where(col>=h[-2],1.,0.).reshape(s,1)))\n",
    "    \n",
    "    for pos_f in positive_features:\n",
    "        pos_f_arr = []\n",
    "        for line in train_data[:,7]:\n",
    "            pos_f_arr.append(np.where(pos_f in line, 1, 0))\n",
    "        bin_train = np.column_stack((bin_train, pos_f_arr))\n",
    "    for neg_f in negative_features:\n",
    "        neg_f_arr = []\n",
    "        for line in train_data[:,7]:\n",
    "            neg_f_arr.append(np.where(neg_f in line, 1, 0))\n",
    "        bin_train = np.column_stack((bin_train, neg_f_arr))\n",
    "\n",
    "    #Binarize Dev Data\n",
    "    dev_data = X[l/2:(3*l)/4]\n",
    "    s1=dev_data.shape[0]\n",
    "    bin_dev=np.where(dev_data[:,0]==u'N/A',0.,1.).reshape(s1,1)\n",
    "\n",
    "    for f, b in fields_bins.items():\n",
    "        col=dev_data[:,f]\n",
    "        h=np.histogram(col,bins=binnum,normed=True)[1]\n",
    "        bin_dev = np.column_stack((bin_dev,np.where(col<h[1],1.,0.).reshape(s1,1)))\n",
    "        for i in range(len(h)-2):\n",
    "            bin_dev = np.column_stack((bin_dev,np.where((col>=h[i+1]) & (col<h[i+2]),1.,0.).reshape(s1,1)))\n",
    "        bin_dev = np.column_stack((bin_dev,np.where(col>=h[-2],1.,0.).reshape(s1,1)))\n",
    "\n",
    "    for pos_f in positive_features:\n",
    "        pos_f_arr = []\n",
    "        for line in dev_data[:,7]:\n",
    "            pos_f_arr.append(np.where(pos_f in line, 1, 0))\n",
    "        bin_dev = np.column_stack((bin_dev, pos_f_arr))\n",
    "    for neg_f in negative_features:\n",
    "        neg_f_arr = []\n",
    "        for line in dev_data[:,7]:\n",
    "            neg_f_arr.append(np.where(neg_f in line, 1, 0))\n",
    "        bin_dev = np.column_stack((bin_dev, neg_f_arr))    \n",
    "        \n",
    "    #Binarize Test Data\n",
    "    test_data = X[(3*l)/4:]\n",
    "    s2=test_data.shape[0]\n",
    "    bin_test=np.where(test_data[:,0]==u'N/A',0.,1.).reshape(s2,1)\n",
    "\n",
    "    \n",
    "    for f, b in fields_bins.items():\n",
    "        col=test_data[:,f]\n",
    "        h=np.histogram(col,bins=binnum,normed=True)[1]\n",
    "        bin_test = np.column_stack((bin_test,np.where(col<h[1],1.,0.).reshape(s2,1)))\n",
    "        for i in range(len(h)-2):\n",
    "            bin_test = np.column_stack((bin_test,np.where((col>=h[i+1]) & (col<h[i+2]),1.,0.).reshape(s2,1)))\n",
    "        bin_test = np.column_stack((bin_test,np.where(col>=h[-2],1.,0.).reshape(s2,1)))\n",
    "    for pos_f in positive_features:\n",
    "        pos_f_arr = []\n",
    "        for line in test_data[:,7]:\n",
    "            pos_f_arr.append(np.where(pos_f in line, 1, 0))\n",
    "        bin_test = np.column_stack((bin_test, pos_f_arr))\n",
    "    for neg_f in negative_features:\n",
    "        neg_f_arr = []\n",
    "        for line in test_data[:,7]:\n",
    "            neg_f_arr.append(np.where(neg_f in line, 1, 0))\n",
    "        bin_test = np.column_stack((bin_test, neg_f_arr))\n",
    "        \n",
    "    global bin_train,bin_dev,bin_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Binarize2() and the mixture_model functions, we see an improvement in accuracy on the dev set up to 85%, but there was no improvement on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive weights:\n",
      "[u'to ask for', u'just your average', u'will pay forward', u'some pizza love', u'really make my day', u'and will be', u'but don think', u'in the title', u'help guy out', u'right now can']\n",
      "\n",
      "negative weights:\n",
      "[u'started new job', u'low on cash', u'like to have', u'would be appreciated', u'food for the next', u'for the first', u'make it to', u'just moved to', u'mother of two', u'are not starving']\n",
      "\n",
      "Top Scoring GMM Models\n",
      "PCA Components  ||  GMM Components  ||  Covariance Type  ||  Accuracy  ||  Parameters\n",
      "            10  ||               1  ||             full  ||  0.8455    ||        26 \n",
      "            12  ||               1  ||             tied  ||  0.8455    ||        30 \n",
      "            12  ||               2  ||             tied  ||  0.8455    ||        60 \n",
      "            10  ||               1  ||             tied  ||  0.8455    ||        26 \n",
      "            12  ||               1  ||             full  ||  0.8455    ||        30 \n",
      "            13  ||               1  ||             tied  ||  0.8446    ||        32 \n",
      "            13  ||               1  ||             full  ||  0.8446    ||        32 \n",
      "            13  ||               2  ||             tied  ||  0.8446    ||        64 \n",
      "            11  ||               1  ||             full  ||  0.8446    ||        28 \n",
      "            11  ||               1  ||             tied  ||  0.8446    ||        28 \n"
     ]
    }
   ],
   "source": [
    "#Run mixture model on second binned train set\n",
    "Binarize2(fields_bins,X)\n",
    "mixture_model(bin_train,bin_dev,bin_test)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
